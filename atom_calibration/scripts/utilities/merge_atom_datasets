#!/usr/bin/env python3

import argparse
import os
import sys
import random
from copy import deepcopy
from pathlib import Path
from colorama import Fore, Style

# Atom imports
from atom_core.dataset_io import (loadResultsJSON,
                                  saveAtomDataset)
from atom_core.utilities import atomError


# -------------------------------------------------------------------------------
# --- MAIN
# -------------------------------------------------------------------------------
def main():
    ap = argparse.ArgumentParser(description='Splits an ATOM dataset json into train and test datasets.')
    ap.add_argument("-json", "--dataset_json", nargs='+',
                    help="Dataset json files to be fused.", type=str, required=True)
    ap.add_argument("-of", "--output_folder", help="Output folder to save the fused dataset.",
                    required=True, type=str)
    ap.add_argument("-ov", "--overwrite", help="Overwrite dataset folder if it exists.", action='store_true',
                    default=False)

    args= vars(ap.parse_known_args()[0])

    if os.path.exists(args['output_folder']) and not args['overwrite']:  # dataset path exists, abort
        print('\n' + Fore.RED + 'Error: Dataset ' + args['output_folder'] +
                ' exists.\nIf you want to replace it add a "--overwrite" flag.' + Style.RESET_ALL + '\n')
        exit(0)

    # ---------------------------------------
    # --- Verify that the datasets are ok
    # ---------------------------------------
    for dataset_path in args['dataset_json']:
        if not os.path.isfile(dataset_path):
            sys.exit(Fore.RED + 'Dataset ' + dataset_path + ' does not exist.' + Style.RESET_ALL)

    # Load datasets 
    dataset_dict = {}
    for dataset_path in args['dataset_json']:
        print('Loading json file ' + dataset_path)
        dataset, _ = loadResultsJSON(dataset_path)
        dataset_dict[dataset_path] = dataset

    fused_dataset = {'_metadata': None,
                     'calibration_config': None,
                     'collections': None,
                     'additional_sensor_data': None,
                     'additional_data': None,
                     'sensors': None,
                     'patterns': None}
    
    for dataset_path, dataset in dataset_dict.items():
        print('Adding dataset ' + dataset_path)

        # Verify metadata consistency
        if fused_dataset['_metadata'] is None:
            fused_dataset['_metadata'] = deepcopy(dataset['_metadata'])
        elif dataset['_metadata'] is not None:
            if dataset['_metadata']['package_name'] != fused_dataset['_metadata']['package_name']:
                atomError(f"Inconsistent package name. Dataset {dataset_path} has package name {dataset['_metadata']['package_name']}, \
                while fused dataset has package name {fused_dataset['_metadata']['package_name']}")
            if dataset['_metadata']['robot_name'] != fused_dataset['_metadata']['robot_name']:
                atomError(f"Inconsistent robot name. Dataset {dataset_path} has robot name {dataset['_metadata']['robot_name']}, \
                    while fused dataset has robot name {fused_dataset['_metadata']['robot_name']}")
            if dataset['_metadata']['version'] != fused_dataset['_metadata']['version']:
                atomError(f"Inconsistent version. Dataset {dataset_path} has version {dataset['_metadata']['version']}, \
                    while fused dataset has version {fused_dataset['_metadata']['version']}")

        # Verify and fuse calibration configuration
        if fused_dataset['calibration_config'] is None:
            fused_dataset['calibration_config'] = deepcopy(dataset['calibration_config'])
        elif dataset['calibration_config'] is not None:
            for config_key, config in dataset['calibration_config'].items():
                if config_key == 'bag_file':
                    continue
                if fused_dataset['calibration_config'][config_key] is None:
                    fused_dataset['calibration_config'][config_key] = deepcopy(config)
                elif config is not None:
                    if isinstance(fused_dataset['calibration_config'][config_key], dict):
                        for sub_key, sub_value in config.items():
                            # TODO this might be too hardcoded for joints
                            if config_key == 'joints' and sub_key in fused_dataset['calibration_config'][config_key]:
                                fused_dataset['calibration_config'][config_key][sub_key]['params_to_calibrate'] =\
                                    list(set(fused_dataset['calibration_config'][config_key][sub_key]['params_to_calibrate'] 
                                    + config[sub_key]['params_to_calibrate'] ))
                            else:
                                if (sub_key in fused_dataset['calibration_config'][config_key] and 
                                sub_value != fused_dataset['calibration_config'][config_key][sub_key]):
                                    atomError(f"{config_key} '{sub_key}' in dataset {dataset_path} has different values than fused dataset")
                                fused_dataset['calibration_config'][config_key][sub_key] = deepcopy(sub_value)
                    elif fused_dataset['calibration_config'][config_key] != dataset['calibration_config'][config_key]:
                        atomError(f"{config_key} in dataset {dataset_path} has different values than fused dataset")

        if fused_dataset['sensors'] is None:
            fused_dataset['sensors'] = deepcopy(dataset['sensors'])
        elif dataset['sensors'] is not None:
            for sensor_key, sensor in dataset['sensors'].items():
                if sensor_key not in fused_dataset['sensors']:
                    fused_dataset['sensors'][sensor_key] = deepcopy(sensor)
                elif fused_dataset['sensors'][sensor_key] != sensor:
                    continue
                    # TODO camera info is stamped, so it will always be different. Need to think of a way to evaluate that
                    atomError(f"{sensor_key} in dataset {dataset_path} has different values than fused dataset")

        if fused_dataset['patterns'] is None:
            fused_dataset['patterns'] = deepcopy(dataset['patterns'])
        elif dataset['patterns'] is not None:
            for pattern_key, pattern in dataset['patterns'].items():
                if pattern_key not in fused_dataset['patterns']:
                    fused_dataset['patterns'][pattern_key] = deepcopy(pattern)
                elif fused_dataset['patterns'][pattern_key] != pattern:
                    atomError(f"{pattern_key} in dataset {dataset_path} has different values than fused dataset")

        if fused_dataset['collections'] is None:
            fused_dataset['collections'] = deepcopy(dataset['collections'])
        elif dataset['collections'] is not None:
            max_collection_key = max(list(map(int, fused_dataset['collections'].keys())))
            for collection in dataset['collections'].values():
                max_collection_key += 1
                if 'contact' not in collection:
                    for sensor in collection['data'].values():
                        # To save as new collection
                        del sensor['data_file']
                fused_dataset['collections'][str(max_collection_key).zfill(3)] = deepcopy(collection)
        
        # Guarantee all collections have all patterns
        for collection in fused_dataset['collections'].values():
            if 'contact' in collection:
                continue

            for pattern_key in fused_dataset['patterns'].keys():
                if pattern_key in collection['labels']:
                    continue

                collection['labels'][pattern_key] = {}
                for sensor_key in fused_dataset['sensors'].keys():
                    collection['labels'][pattern_key][sensor_key] = {'detected': False, 'idxs': [], 'idxs_limit_points': []}


        if fused_dataset['additional_data'] is None and 'additional_data' in dataset:
            fused_dataset['additional_data'] = deepcopy(dataset['additional_data'])
        elif 'additional_data' in dataset and dataset['additional_data'] is not None:
            for additional_data_key, additional_data in dataset['additional_data'].items():
                if additional_data_key not in fused_dataset['additional_data']:
                    fused_dataset['additional_data'][additional_data_key] = deepcopy(additional_data)
                elif fused_dataset['additional_data'][additional_data_key] != additional_data:
                    atomError(f"{additional_data_key} in dataset {dataset_path} has different values than fused dataset")


    # Create folder for dataset
    os.makedirs(args['output_folder'], exist_ok=True)
    saveAtomDataset(args['output_folder'] + '/dataset.json', fused_dataset)

            
                

if __name__ == "__main__":
    main()
