#!/usr/bin/env python3

# stdlib
import os
import sys
import argparse
import subprocess
import itertools
from os.path import exists
from copy import deepcopy
from datetime import date, datetime

import matplotlib
import atom_core.config_io
import atom_core.drawing

# 3rd-party
import yaml
import numpy
import rospy
import rospkg
import rosbag
import jinja2
import matplotlib.pyplot as plt
import networkx as nx
from pytictoc import TicToc

# local packages
from atom_core.utilities import atomPrintOK, checkDirectoryExistence, atomError, atomStartupPrint, verifyAnchoredSensor
from atom_core.naming import generateLabeledTopic
from colorama import Style, Fore
from graphviz import Digraph
from urdf_parser_py.urdf import URDF
from matplotlib import cm
from jinja2 import Environment, FileSystemLoader


# TODO but if modality is called lidar3d we should not create an alternative name. This is confusing ...
def getModalityEvaluation(sensor):
    # Helper function to simplify modality extraction
    return 'lidar' if sensor['modality'] == 'lidar3d' else sensor['modality']


if __name__ == "__main__":
    # Parse command line arguments
    ap = argparse.ArgumentParser()
    ap.add_argument("-n", "--name", help='package name', type=str, required=True)
    ap.add_argument("-utf", "--use_tfs", action="store_true",
                    help='Use transformations in the bag file instead of generating new tfs from the xacro, '
                         'joint_state_msgs and robot state publisher.')
    ap.add_argument("-cgt", "--collect_ground_truth", action="store_true",
                    help='Assume transformations (\\tf and \\tf_static) in the bag file to be perfect, and collect them as ground truth for ATOM datasets. Useful for evaluating the calibration against ground truth.')
    ap.add_argument("-cfg", "--config_file", help='Specify if you want to configure the calibration package with a specific configuration file. If this flag is not given, the standard config.yml ill be used.',
                    type=str, required=False, default=None)
    args = vars(ap.parse_args())

    # --------------------------------------------------------------------------
    # Initial setup
    # --------------------------------------------------------------------------
    tictoc = TicToc()
    tictoc.tic()

    package_name = os.path.basename(args['name'])
    atomStartupPrint('Configuring calibration package ' + Fore.BLUE + package_name + Style.RESET_ALL)

    rospack = rospkg.RosPack()
    atom_calibration_path = rospack.get_path('atom_calibration')

    if not package_name in rospack.list():  # Check if package is under $ROS_PACKAGE_PATH, abort if not
        atomError('ROS package ' + Fore.BLUE + package_name + Style.RESET_ALL + ' not found under ROS. Are you sure the package in under a directory listed in $ROS_PACKAGE_PATH? Can you run:\n\n' +
                  Fore.BLUE + 'roscd ' + package_name + Style.RESET_ALL + '\n\nPlease fix this before running your package configuration.')

    package_path = rospack.get_path(package_name)  # full path to the package, including its name.
    package_base_path = os.path.dirname(package_path)  # parent path where the package is located

    rviz_file_template = atom_calibration_path + '/templates/config.rviz'
    rviz_set_initial_estimate = 'set_initial_estimate.rviz'
    rviz_collect_data = 'collect_data.rviz'
    rviz_calibrate = 'calibrate.rviz'
    rviz_dataset_playback = 'dataset_playback.rviz'
    playbag_launch_file = package_path + '/launch/playbag.launch'
    set_initial_estimate_launch_file = package_path + '/launch/set_initial_estimate.launch'
    data_collection_launch_file = package_path + '/launch/collect_data.launch'
    calibrate_launch_file = package_path + '/launch/calibrate.launch'
    dataset_playback_launch_file = package_path + '/launch/dataset_playback.launch'
    full_evaluation_launch_file = package_path + '/launch/full_evaluation.launch'

    # Verify if the standard atom_calibration directories exist, if not create them (#401)
    checkDirectoryExistence('calibration', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('launch', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('rviz', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('scripts', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('urdf', package_name, create_if_nonexistent=True)

    # Template engine1 setup
    file_loader = FileSystemLoader(atom_calibration_path + '/templates')
    env = Environment(loader=file_loader, undefined=jinja2.StrictUndefined)
    env.add_extension('jinja2.ext.do')

    # Date
    dt_string = datetime.now().strftime("%d/%m/%Y %H:%M:%S")

    # --------------------------------------------------------------------------
    # Read the config.yml file
    # --------------------------------------------------------------------------
    if args['config_file'] is None:
        args['config_file'] = package_path + '/calibration/config.yml'
    else:
        args['config_file'] = package_path + '/calibration/' + args['config_file']
        if not exists(args['config_file']):
            args['config_file'] = package_path + '/calibration/config.yml'

    print('Loading config_file ' + Fore.BLUE + str(args['config_file']) + Style.RESET_ALL)
    config = atom_core.config_io.loadConfig(args['config_file'])

    # Sensors colormap. Access with:  color_map_sensors[idx, :]
    cm_sensors = cm.Set3(numpy.linspace(0, 1, len(config['sensors'].keys())))

    # --------------------------------------------------------------------------
    # Read the bag file
    # --------------------------------------------------------------------------
    bag_file, _, bag_file_rel = atom_core.config_io.uriReader(config['bag_file'])
    print('Loading bagfile ' + Fore.BLUE + bag_file + Style.RESET_ALL)
    bag = rosbag.Bag(bag_file)  # load the bag file
    bag_info = bag.get_type_and_topic_info()
    bag_types = bag_info[0]
    bag_topics = bag_info[1]
    # print('\n' + str(bag_topics))

    # --------------------------------------------------------------------------
    # Setup the description file
    # --------------------------------------------------------------------------
    description_file, _, _ = atom_core.config_io.uriReader(config['description_file'])
    description_file_out_initial_estimate = package_path + '/urdf/initial_estimate.urdf.xacro'
    atom_core.config_io.execute('cp ' + description_file + ' ' + description_file_out_initial_estimate,
                                verbose=False)  # Copy the xacro to the initial_estimate file

    # Check the description file
    urdf_file = '/tmp/description.urdf'
    if os.path.exists(urdf_file):
        # print('Deleting temporary file ' + urdf_file)
        os.remove(urdf_file)

    print('Parsing description file ' + Fore.BLUE + description_file + Style.RESET_ALL)
    xacro_cmd = 'xacro ' + description_file + ' -o ' + urdf_file
    atom_core.config_io.execute(xacro_cmd, verbose=True)  # create tmp urdf file

    if not os.path.exists(urdf_file):
        atomError('Could not parse description file ' + Fore.BLUE + description_file + Style.RESET_ALL + '\nYou must manually run command:\n' +
                  Fore.BLUE + xacro_cmd + Style.RESET_ALL + '\nand fix the problem before configuring your calibration package.')

    description = URDF.from_xml_file(urdf_file)  # read the urdf file

    # --------------------------------------------------------------------------
    # Check if modalities are properly created
    # --------------------------------------------------------------------------
    # TODO change lidar to lidar3d and lidar2d to lidar2d; change modality to modality
    for sensor_key in config['sensors']:
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        if topic in bag_topics:
            msg_type = bag_info[1][topic].msg_type
        elif topic_compressed in bag_topics:
            msg_type = bag_info[1][topic_compressed].msg_type
        if ('modality' not in config['sensors'][sensor_key]) or (
                config['sensors'][sensor_key]['modality'] not in ['rgb', 'depth', 'lidar3d', 'lidar2d']):
            print(Fore.YELLOW + 'Warning: Sensor ID not properly identified for sensor ' + sensor_key + ' with topic '
                  + topic + "Sensor will be identified by message type." + Style.RESET_ALL)
            if msg_type == 'sensor_msgs/CompressedImage' or msg_type == 'sensor_msgs/Image':
                config['sensors'][sensor_key]['modality'] = 'rgb'
            elif msg_type == 'sensor_msgs/LaserScan':
                config['sensors'][sensor_key]['modality'] = 'lidar2d'
            elif msg_type == 'sensor_msgs/PointCloud2':
                config['sensors'][sensor_key]['modality'] = 'lidar3d'
            else:
                atomError('Cannot generate rviz configuration for sensor ' + Fore.BLUE + sensor_key +
                          Style.RESET_ALL + ' with topic ' + Fore.BLUE + topic + Style.RESET_ALL)

    # --------------------------------------------------------------------------
    # Create a tf graph to support verifications
    # --------------------------------------------------------------------------
    gx = nx.DiGraph()
    if not args['use_tfs']:  # create a graph using the information in the urdf
        print('Creating transformation tree using the urdf robot description ...')
        for link in description.links:  # A graph node for each link in the urdf
            gx.add_node(link.name)

        for joint in description.joints:  # atomic transformations are given by the joints
            if joint.type == 'fixed':
                gx.add_edge(joint.parent, joint.child, weight=1, type='static')
            else:
                gx.add_edge(joint.parent, joint.child, weight=1, type='dynamic')
    else:
        print('Creating transformation tree using the tfs in the bagfile ...')

        for topic, msg, t in bag.read_messages(topics=['/tf']):
            for transform in msg.transforms:
                if not gx.has_edge(transform.header.frame_id.replace('/', ''), transform.child_frame_id.replace('/', '')):
                    # print(transform.header.frame_id, transform.child_frame_id)
                    gx.add_edge(transform.header.frame_id, transform.child_frame_id, weight=1, type='dynamic')

        for topic, msg, t in bag.read_messages(topics=['/tf_static']):
            for transform in msg.transforms:
                if not gx.has_edge(transform.header.frame_id.replace('/', ''), transform.child_frame_id.replace('/', '')):
                    # print(transform.header.frame_id.replace('/',''), transform.child_frame_id.replace('/',''))
                    gx.add_edge(transform.header.frame_id.replace('/', ''),
                                transform.child_frame_id.replace('/', ''), weight=1, type='static')

    # nx.draw(gx, with_labels=True)
    # plt.show()

    # --------------------------------------------------------------------------
    # Verifications: Run as much as we can think of to see if something is wrong. Better early than late.
    # --------------------------------------------------------------------------
    print('Running several verifications ... please wait ...')

    # If anchored sensor exists, it must be one of the existing sensors
    verifyAnchoredSensor(config['anchored_sensor'], config['sensors'])

    # Check if sensor names are valid:
    sensor_keys = list(config['sensors'].keys())

    for i in range(len(sensor_keys)):
        for j in range(i + 1, len(sensor_keys)):
            key1 = sensor_keys[i]
            key2 = sensor_keys[j]
            if key1 in key2:
                atomError(f"'{key1}' is a string subset of '{key2}', please rename them")
            elif key2 in key1:
                atomError(f"'{key2}' is a string subset of '{key1}', please rename them")

    atomPrintOK()

    # Check if config sensor topics exist in the bag file (also check for compressed topics)
    print('Checking for compressed image topics ... ')
    compressed_topics = {}  # a list of compressed topics to decompress in the launch file
    for sensor_key in config['sensors']:
        topic = config['sensors'][sensor_key]['topic_name']
        if topic not in bag_topics:

            topic_compressed = topic + '/compressed'
            if topic_compressed in bag_topics:  # Check if the topic is a compressed image topic
                msg_type = bag_info[1][topic_compressed].msg_type
                if msg_type == 'sensor_msgs/CompressedImage':  # Check if the topic of correct msg_type
                    compressed_topics[topic] = {'topic_compressed': topic_compressed, 'msg_type': msg_type,
                                                'sensor_key': sensor_key}
                    print('Topic ' + Fore.BLUE + topic + Style.RESET_ALL +
                          ' is in compressed format (' + topic_compressed + '). Will setup a decompressor.')
                else:
                    atomError('Topic ' + Fore.BLUE + topic + Style.RESET_ALL + ' (from sensor ' + Fore.BLUE + sensor_key +
                              ') exists in compressed format in the bag file, but is not of msg_type "sensor_msgs/CompressedImage"' + Style.RESET_ALL)
            else:
                raise atomError('Topic ' + Fore.BLUE + topic + Style.RESET_ALL + ' (from sensor ' + Fore.BLUE +
                                sensor_key + Style.RESET_ALL + ') does not exist in the bag file.' + Style.RESET_ALL)

    # Check if throttled topics exist
    print('Checking for throttled topics ... ')
    throttle_topics = {}
    use_throttle_topics = {}
    for sensor_key in config['sensors']:
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        if topic_compressed in bag_topics:  # Check if the topic is a compressed image topic
            topic = topic_compressed
        if 'throttle' in config['sensors'][sensor_key]:
            throttle = config['sensors'][sensor_key]['throttle']
            print('Sensor ' + sensor_key + ' throttled to ' + Fore.BLUE +
                  str(config['sensors'][sensor_key]['throttle']) + Style.RESET_ALL + ' Hz.')
            use_throttle = True
        else:
            throttle = None
            use_throttle = False

        throttle_topics[topic] = {'throttled_topic': topic, 'sensor_key': sensor_key, 'use_throttle': use_throttle,
                                  'throttle_hz': throttle}

    # For rgb and depth sensors, check that the camera_info topic exists in the bag file
    print('Checking for camera_info topic in bagfile ... ', end='')
    for sensor_key in config['sensors']:
        # no need to check for other modalities
        if not config['sensors'][sensor_key]['modality'] == 'rgb' and not config['sensors'][sensor_key]['modality'] == 'depth':
            continue

        topic = config['sensors'][sensor_key]['topic_name']
        camera_info_topic = os.path.dirname(topic) + '/camera_info'
        if camera_info_topic not in bag_topics:
            atomError('Topic ' + Fore.BLUE + camera_info_topic + Style.RESET_ALL + ' (from sensor ' + Fore.BLUE + sensor_key +
                      Style.RESET_ALL + ') does not exist in the bag file. This topic must exist. Aborting configuration!' + Style.RESET_ALL)

    atomPrintOK()

    # Verify the existence of all tf frames referred in the config file
    print('Checking if coordinate frames from config.yml exist in bagfile ... ', end='')
    frames_to_verify = {'world_link': config['world_link'],
                        'calibration_pattern.pattern_link': config['calibration_pattern']['parent_link']}
    for sensor_key in config['sensors']:  # Check if link, child_link and parent_link exist in the bagfile.
        for frame in ['child_link', 'parent_link', 'link']:
            frames_to_verify[sensor_key + '.' + frame] = config['sensors'][sensor_key][frame]

    # Search for additional tfs to be estimated
    if 'additional_tfs' in config:
        if config['additional_tfs'] != '':
            # Check if link, child_link and parent_link exist in the bagfile.
            for additional_tf_key in config['additional_tfs']:
                for frame in ['child_link', 'parent_link']:
                    frames_to_verify[additional_tf_key + '.' +
                                     frame] = config['additional_tfs'][additional_tf_key][frame]

            # Verify that the parent and child links for each additional_tfs are a direct transformation
            for additional_tf_key, additional_tf in config['additional_tfs'].items():
                edge_in_config = (config['additional_tfs'][additional_tf_key]['parent_link'],
                                  config['additional_tfs'][additional_tf_key]['child_link'])

                if not edge_in_config in gx.edges():
                    atomError('Additional_tfs ' + Fore.BLUE + additional_tf_key + Style.RESET_ALL + ' parent and child links are ' + Fore.YELLOW +
                              edge_in_config[0] + Fore.BLACK + ' and ' + Fore.YELLOW + edge_in_config[1] + Fore.BLACK + ' but there is no such atomic transformation in the tf tree.' + Style.RESET_ALL)

            for additional_tf_key, additional_tf in config['additional_tfs'].items():

                # Check if frames exits
                if additional_tf['child_link'] not in gx.nodes:
                    atomError('Additional tf for child link ' + Fore.BLUE + additional_tf['child_link'] + Style.RESET_ALL + ' not found.\nCheck the frames you entered in the config.yaml. Consider using the ' +
                              Fore.GREEN + '-utf/--use_tfs' + Style.RESET_ALL + ' option to use the tranforms on the /tf and /tf_static topics.')

                # path between root and additional_tfs data frame
                print('\nChecking path from ' + config['world_link'] + ' to ' + additional_tf['child_link'])
                paths = nx.shortest_path(gx, config['world_link'], additional_tf['child_link'])

                print('Path from ' + Fore.RED + config['world_link'] + Fore.RESET +
                      ' to additional_tfs ' + Fore.LIGHTMAGENTA_EX + additional_tf_key + Fore.RESET + ':')
                at = '['
                for path in paths:
                    if path == config['world_link']:
                        at += Fore.RED + path + Fore.RESET + ', '
                    elif path == additional_tf['parent_link'] or path == additional_tf['child_link']:
                        at += Fore.BLUE + path + Fore.RESET + ', '
                    else:
                        for joint in description.joints:  # atomic transformations are given by the joints
                            if path == joint.parent or path == joint.child:
                                if joint.type == 'fixed':
                                    at += path + ', '
                                else:
                                    at += Fore.GREEN + path + Fore.RESET + ', '
                                break
                at = at[:-2]
                at += ']'
                print(at)

    # Checking if all tf frames exist
    for key, frame in frames_to_verify.items():
        if not frame in gx:
            atomError('Frame ' + Fore.BLUE + frame + Style.RESET_ALL + ' given in config file for ' +
                      Fore.BLUE + key + Style.RESET_ALL + ' does not exist. Check config file ' +
                      Fore.BLUE + args['config_file'] + Style.RESET_ALL)
    atomPrintOK()

    # Verify that the parent and child links for each sensor are a direct transformation (#316).
    print('Checking if transforms in config.yml are direct transformations ... ', end='')
    for sensor_key, sensor in config['sensors'].items():
        edge_in_config = (config['sensors'][sensor_key]['parent_link'], config['sensors'][sensor_key]['child_link'])
        if not edge_in_config in gx.edges():
            raise ValueError(
                'Error: sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL + ' parent and child links are ' + Fore.YELLOW +
                edge_in_config[0] + Fore.BLACK + ' and ' + Fore.YELLOW + edge_in_config[1] + Fore.BLACK +
                ' but there is no such atomic transformation in the tf tree.' + Style.RESET_ALL)
    atomPrintOK()

    # Verify that the frame_id in the calibration.yaml->sensor->link field is the same as in the published messages (#514).
    print('Checking consistency of configuration tf frames with the messages on corresponding topic ...', end='')
    for sensor_key, sensor in config['sensors'].items():
        frame_in_config = config['sensors'][sensor_key]['link']
        topic_in_config = config['sensors'][sensor_key]['topic_name']

        for topic, msg, t in bag.read_messages(topics=[topic_in_config]):
            if not msg.header.frame_id == frame_in_config:
                atomError('Sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL + ' calibration config defined link ' + Fore.YELLOW + frame_in_config + Fore.BLACK + ' but bagfile messages on topic ' +
                          Fore.BLUE + topic_in_config + Fore.BLACK + ' have header.frame_id ' + Fore.YELLOW + msg.header.frame_id + Fore.BLACK + '. These should be the same! Check #514.' + Style.RESET_ALL)
                exit(0)
            break  # check consistency only for the first message
    atomPrintOK()

    # if nx.is_connected(gx):
    if nx.is_weakly_connected(gx):
        print('Checking if TF tree is connected ... ', end='')
    else:
        pos = nx.random_layout(gx)
        edges, weights = zip(*nx.get_edge_attributes(gx, 'weight').items())
        edge_labels = nx.draw_networkx_edge_labels(gx, pos)
        nx.draw(gx, with_labels=True)
        plt.show()
        raise ValueError('TF tree is not connected. Aborting.')

    atomPrintOK()

    # Pretty print of transformation chains
    # TODO How to detect a dynamic transformation using the information in the bagfile's tfs?
    print(
        'Transformations for sensors. Color coding is ' + Fore.RED +
        'root link' + Fore.RESET + ', ' + Fore.BLUE + 'to be calibrated'
        + Fore.RESET + ', ' + Fore.GREEN + 'dynamic transformation' + Fore.RESET + ', ' + Fore.LIGHTMAGENTA_EX +
        'sensor data frame' + Fore.RESET + '.')
    for sensor_key, sensor in config['sensors'].items():
        paths = nx.shortest_path(gx, config['world_link'], sensor['link'])  # path between root and sensor data frame
        print('Path from ' + Fore.RED + config['world_link'] + Fore.RESET + ' to sensor ' + Fore.LIGHTMAGENTA_EX +
              sensor_key + Fore.RESET + ':')
        s = '['
        for path in paths:
            if path == config['world_link']:
                s += Fore.RED + path + Fore.RESET + ', '
            elif path == sensor['parent_link'] or path == sensor['child_link']:
                s += Fore.BLUE + path + Fore.RESET + ', '
            elif path == sensor['link']:
                s += Fore.LIGHTMAGENTA_EX + path + Fore.RESET + ', '
            else:
                for joint in description.joints:  # atomic transformations are given by the joints
                    if path == joint.parent or path == joint.child:
                        if joint.type == 'fixed':
                            s += path + ', '
                        else:
                            s += Fore.GREEN + path + Fore.RESET + ', '
                        break
        s = s[:-2]
        s += ']'
        print(s)

    # Verify that the transformations selected to be optimized are indeed static (#581)
    for sensor_key, sensor in config['sensors'].items():
        parent = sensor['parent_link']
        child = sensor['child_link']
        edge = gx.get_edge_data(parent, child)

        if not edge['type'] == 'static':
            print('Error: sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL + ' defines transformation ' + Fore.LIGHTMAGENTA_EX + parent +
                  ' to ' + child + Style.RESET_ALL + ' to be calibrated but this transformation is not static and therefore cannot be calibrated.')
            exit(0)

    if 'additional_tfs' in config and config['additional_tfs']:  # must exist and not be null
        for additional_tf_key, additional_tf in config['additional_tfs'].items():
            parent = additional_tf['parent_link']
            child = additional_tf['child_link']
            edge = gx.get_edge_data(parent, child)

            if not edge['type'] == 'static':
                print('Error: additional_tf  ' + Fore.BLUE + additional_tf_key + Style.RESET_ALL + ' defines transformation ' + Fore.LIGHTMAGENTA_EX +
                      parent + ' to ' + child + Style.RESET_ALL + ' to be calibrated but this transformation is not static and therefore cannot be calibrated.')
                exit(0)

    # --------------------------------------------------------------------------
    # Create dot calibration graph (used for printing out a summary)
    # --------------------------------------------------------------------------
    g = Digraph('G', filename='summary', directory='/tmp/', graph_attr={'title': 'graph'})  # create a graph

    for link in gx.nodes:  # A graph node for each link in the urdf
        rgb = matplotlib.colors.rgb2hex([0, 0, 0])  # black by default
        label = link
        if config['world_link'] == link:
            label += '\n(world link)'
            rgb = matplotlib.colors.rgb2hex([1, 0, 0])

        for sensor_key, sensor in config['sensors'].items():
            if sensor['link'] == link:
                label += '\n(data from sensor ' + sensor_key + ')'
                rgb = matplotlib.colors.rgb2hex([0, 0.8, 0])

        g.node(link, label=label, _attributes={'penwidth': '2', 'color': rgb}, )

    # Add node with the pattern link
    rgb = matplotlib.colors.rgb2hex([0.8, 0.8, 0])
    g.node(config['calibration_pattern']['link'], label=' ' + config['calibration_pattern']['link'] + ' \n (calibration pattern) ',
           _attributes={'penwidth': '2', 'color': rgb}, )

    for parent, child, edge_data in gx.edges(data=True):  # atomic transformations are given by the joints
        label = ''
        type = edge_data['type']
        rgb = matplotlib.colors.rgb2hex([0, 0, 0])  # black by default
        to_be_calibrated = False  # If this transform is to be calibrated, change the label on the edge

        for sensor_key, sensor in config['sensors'].items():  # if the joint is marked for calibration
            if sensor['parent_link'] == parent and sensor['child_link'] == child:
                label += ' To be calibrated \n'
                rgb = matplotlib.colors.rgb2hex([0, 0, 1])
                to_be_calibrated = True

        if 'additional_tfs' in config:
            if config['additional_tfs'] != '':
                for additional_tf_key, additional_tf in config['additional_tfs'].items():
                    if additional_tf['parent_link'] == parent and additional_tf['child_link'] == child:
                        label += ' To be calibrated \n'
                        rgb = matplotlib.colors.rgb2hex([0, 0, 1])
                        to_be_calibrated = True

        if not to_be_calibrated:
            label += ' ' + type + ' '  # add word "static" or "dynamic"
        else:
            if type == 'static':
                label += ' (single transform) '
            else:
                label += ' (multiple transforms) '

        g.edge(parent, child, color=rgb, style='solid', _attributes={'penwidth': '1', 'fontcolor': rgb},
               label=label)

    # Add edge with the connection to the pattern link
    if config['calibration_pattern']['fixed']:
        label = ' To be calibrated \n (single transform) '
    else:
        label = ' To be calibrated \n (multiple transforms) '

    rgb = matplotlib.colors.rgb2hex([0, 0, 1])  # blue for dynamic
    g.edge(config['calibration_pattern']['parent_link'],
           config['calibration_pattern']['link'], color=rgb, style='solid',
           _attributes={'penwidth': '1', 'fontcolor': rgb},
           label=label)

    # g.view()
    g.render(filename='summary', directory=package_path + '/calibration', cleanup=True)

    # --------------------------------------------------------------------------
    # Create the playback launch file
    # --------------------------------------------------------------------------
    launch_file = playbag_launch_file
    print('Setting up ' + Fore.BLUE + playbag_launch_file + Style.RESET_ALL + ' ...')

    template = env.get_template('playbag.launch')
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'bag_file': bag_file_rel,
                                    'use_tfs': args['use_tfs'],
                                    'collect_ground_truth': args['collect_ground_truth'],
                                    'package_name': package_name,
                                    'rviz_set_initial_estimate': rviz_set_initial_estimate,
                                    'use_compressed_topics': bool(compressed_topics),
                                    'compressed_topics': compressed_topics,
                                    'throttle_topics': throttle_topics
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the set_initial_estimate launch file
    # --------------------------------------------------------------------------
    launch_file = set_initial_estimate_launch_file
    print('Setting up ' + Fore.BLUE + launch_file + Style.RESET_ALL + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'rviz_set_initial_estimate': rviz_set_initial_estimate,
                                    'bag_file': bag_file_rel,
                                    'marker_size': 0.5,
                                    'config_file': os.path.basename(args['config_file'])
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the data collection launch file
    # --------------------------------------------------------------------------
    launch_file = data_collection_launch_file
    print('Setting up ' + Fore.BLUE + launch_file + Style.RESET_ALL + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'rviz_collect_data': rviz_collect_data,
                                    'bag_file': bag_file_rel,
                                    'config_file': os.path.basename(args['config_file'])
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the calibrate launch file
    # --------------------------------------------------------------------------
    launch_file = calibrate_launch_file
    print('Setting up ' + Fore.BLUE + launch_file + Style.RESET_ALL + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'rviz_calibrate': rviz_calibrate
                                    })
        f.write(output.encode('utf-8'))
    # --------------------------------------------------------------------------
    # Create the dataset playback launch file
    # --------------------------------------------------------------------------
    launch_file = dataset_playback_launch_file
    print('Setting up ' + Fore.BLUE + launch_file + Style.RESET_ALL + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'bag_file': bag_file_rel,
                                    'rviz_dataset_playback': rviz_dataset_playback
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the full evaluation launch file
    # --------------------------------------------------------------------------
    launch_file = full_evaluation_launch_file
    print('Setting up ' + Fore.BLUE + launch_file + Style.RESET_ALL + ' ...')

    # Create a list of all possible combinations of sensor pairs
    sensor_combinations_list = list(itertools.combinations(config['sensors'].keys(), 2))
    # Initialize a dictionary to store sensor combination descriptions
    sensor_combinations_dict = dict()

    # Define mappings for filenames and argumets based on sensor modalities
    modality_to_filename = {
        ('rgb', 'lidar'): 'lidar_to_rgb',
        ('depth', 'lidar'): 'lidar_to_depth',
        ('rgb', 'depth'): 'depth_to_rgb',
    }

    modality_to_args = {
        ('rgb', 'depth'): '-cs {} -ds {}',
        ('depth', 'rgb'): '-ds {} -cs {}',
        ('rgb', 'lidar'): '-cs {} -rs {}',
        ('lidar', 'rgb'): '-rs {} -cs {}',
        ('depth', 'lidar'): '-ds {} -rs {}',
        ('lidar', 'depth'): '-rs {} -ds {}',
    }

    for source, target in sensor_combinations_list:
        # Initialize a description for the evaluation
        evaluation_description = {
            'filename': None,
            'args': None,
        }

        # Determine the modalities of the source and target sensors
        source_modality = getModalityEvaluation(config['sensors'][source])
        target_modality = getModalityEvaluation(config['sensors'][target])
        modality_combination = (source_modality, target_modality)

        # Set the filename and args based on the modality combination
        if modality_combination in modality_to_filename:
            evaluation_description['filename'] = modality_to_filename[modality_combination]
        else:
            evaluation_description['filename'] = '{}_to_{}'.format(source_modality, target_modality)

        if source_modality == target_modality:
            evaluation_description['args'] = f'-ss {source} -st {target}'
        else:
            evaluation_description['args'] = modality_to_args[modality_combination].format(source, target)

        # Store the evaluation description in the sensor_combinations_dict
        sensor_combinations_dict[source + '_to_' + target] = evaluation_description

        # Only valid when sensor is fixed
        if config['calibration_pattern']['fixed']:
            # Create an inter-collection evaluation description
            inter_collection_evaluation_description = {
                'filename': None,
                'args': None,
            }

            # Set the filename and arguments for inter-collection evaluation
            inter_collection_evaluation_description['filename'] = 'inter_collection_' + \
                evaluation_description['filename']
            inter_collection_evaluation_description['args'] = evaluation_description['args'] + \
                ' -wf ' + config['world_link']

            # Store the inter-collection evaluation description in the sensor_combinations_dict
            sensor_combinations_dict['inter_collection_' + source +
                                     '_to_' + target] = inter_collection_evaluation_description

    # Only valid when sensor is fixed
    if config['calibration_pattern']['fixed']:
        # Create evaluation descriptions for sensors against themselves
        for sensor in config['sensors'].keys():
            modality = getModalityEvaluation(config['sensors'][sensor])
            evaluation_description = {
                'filename': None,
                'args': None,
            }

            evaluation_description['filename'] = 'inter_collection_' + modality + '_to_' + modality
            evaluation_description['args'] = f'-ss {sensor} -st {sensor} -wf {config["world_link"]}'

            sensor_combinations_dict['inter_collection_' + sensor + '_to_' + sensor] = evaluation_description

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'sensor_combinations': sensor_combinations_dict,
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the rviz config core displays (used in several rviz config files)
    # --------------------------------------------------------------------------
    # TODO change rviz fixed_frame according to args['world_link']
    core_displays = []

    # Create grid, tf and robot model displays
    rendered = env.get_template('/rviz/Grid.rviz').render(c={'Reference_Frame': config['world_link']})
    core_displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    rendered = env.get_template('/rviz/TF.rviz').render(c={'Name': 'TF'})
    core_displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    rendered = env.get_template('/rviz/RobotModel.rviz').render(c={'Name': 'RobotModel'})
    core_displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the set_initial_estimate
    # --------------------------------------------------------------------------
    print('Setting up ' + Fore.BLUE + rviz_set_initial_estimate + Style.RESET_ALL + ' ...')

    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays

    # Create interactive marker display for moving the sensors
    rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(c={'Name': 'MoveSensors-InteractiveMarker',
                                                                          'Update_Topic': 'set_initial_estimate/update'}
                                                                       )
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Generate rviz displays according to the sensor types
    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        # if topic in bag_topics:
        #     msg_type = bag_info[1][topic].msg_type
        # else:
        #     msg_type = bag_info[1][topic_compressed].msg_type
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL +
              ' with topic ' + Fore.BLUE + topic + Style.RESET_ALL + ' (' + Fore.CYAN + modality + Style.RESET_ALL + ')')

        # if msg_type == 'sensor_msgs/CompressedImage' or \
        #         msg_type == 'sensor_msgs/Image':  # add displays for camera sensor
        # TODO check if depth should be here or not
        if modality == 'rgb' or modality == 'depth':

            # Raw image
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Image',
                                                                      'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Camera
            rendered = env.get_template('/rviz/Camera.rviz').render(c={'Name': sensor_key + '-Camera',
                                                                       'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Frustum
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Frustum',
                                                                            'Marker_Topic': sensor_key + '/frustum'})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/LaserScan':
        elif modality == 'lidar2d':
            # Raw data
            rendered = env.get_template('/rviz/LaserScan.rviz').render(c={'Name': sensor_key + '-LaserScan',
                                                                          'Topic': topic,
                                                                          'Color': color})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/PointCloud2':
        elif modality == 'lidar3d':
            # Raw data
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-PointCloud2',
                                                                            'Topic': topic,
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.02,
                                                                            'Alpha': 1})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        else:
            atomError('Warning: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                      + topic + ' (' + modality + ')' + Style.RESET_ALL)

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_set_initial_estimate, 'w'))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the collect_data
    # --------------------------------------------------------------------------
    print('Setting up ' + Fore.BLUE + rviz_collect_data + Style.RESET_ALL + ' ...')

    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays

    # Add interactive maker display to handle sensor manual labeling
    rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
        c={'Name': 'ManualDataLabeler-InteractiveMarkers', 'Update_Topic': 'data_labeler/update'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))
    # TODO this will be the menu (only one)

    depth = 0
    # Generate rviz displays according to the sensor types
    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        # if topic in bag_topics:
        #     msg_type = bag_info[1][topic].msg_type
        # else:
        #     msg_type = bag_info[1][topic_compressed].msg_type
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL +
              ' with topic ' + Fore.BLUE + topic + Style.RESET_ALL + ' (' + Fore.CYAN + modality + Style.RESET_ALL + ')')

        # if msg_type == 'sensor_msgs/CompressedImage' or msg_type == 'sensor_msgs/Image':  # displays for camera sensor
        # TODO check if depth should be here or separated
        if modality == 'rgb':
            # Image
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': topic + '/labeled'})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Camera
            rendered = env.get_template('/rviz/Camera.rviz').render(c={'Name': sensor_key + '-Camera',
                                                                       'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'depth':
            depth += 1
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': topic + '/labeled'})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Camera
            rendered = env.get_template('/rviz/Camera.rviz').render(c={'Name': sensor_key + '-Camera',
                                                                       'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
                c={'Name': sensor_key + '-ManualDataLabeler-InteractiveMarkers',
                   'Update_Topic': sensor_key + '/data_labeler/update'}
            )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/LaserScan':
        elif modality == 'lidar2d':
            rendered = env.get_template('/rviz/LaserScan.rviz').render(c={'Name': sensor_key + '-LaserScan',
                                                                          'Topic': topic,
                                                                          'Color': color})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Labels
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-Labels-PointCloud2',
                                                                            'Topic': topic + '/labeled',
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.2,
                                                                            'Alpha': 0.05})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # TODO Add markers to display data clusters

            rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
                c={'Name': sensor_key + '-ManualDataLabeler-InteractiveMarkers',
                   'Update_Topic': sensor_key + '/data_labeler/update'}
            )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/PointCloud2':
        elif modality == 'lidar3d':
            # Raw data
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-PointCloud2',
                                                                            'Topic': topic,
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.02,
                                                                            'Alpha': 1})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Labeled data
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-Labels-PointCloud2',
                                                                            'Topic': topic + '/labeled',
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.2,
                                                                            'Alpha': 0.05})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Interactive marker for the labeler
            rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
                c={'Name': sensor_key + '-ManualDataLabeler-InteractiveMarkers',
                   'Update_Topic': sensor_key + '/data_labeler/update'}
            )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        else:
            print(Fore.YELLOW + 'Warning: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                  + topic + ' (' + modality + ')' + Style.RESET_ALL)

    if depth != 0:  # TODO #411 this is strange here ...
        # rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        #     c={'Name': 'FrustumVisualizationDepthSensors',
        #        'Marker_Topic': '/sensor_frustum'})
        # displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))
        pass

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_collect_data, 'w'))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the dataset_playback
    # --------------------------------------------------------------------------
    print('Setting up ' + Fore.BLUE + rviz_dataset_playback + Style.RESET_ALL + ' ...')
    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays
    displays.pop()  # remove last core display which is robot model

    rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': 'Robot Meshes',
                                                                    'Marker_Topic': '/dataset_playback/robot_meshes'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL +
              ' with topic ' + Fore.BLUE + topic + Style.RESET_ALL + ' (' + Fore.CYAN + modality + Style.RESET_ALL + ')')

        if modality == 'rgb':
            # Raw image
            labeled_topic = generateLabeledTopic(topic, type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar2d':
            # Raw data
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/LaserScan.rviz').render(c={'Name': sensor_key + '-Labels-LaserScan',
                                                                          'Topic': labeled_topic,
                                                                          'Color': color})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar3d':
            # Raw data
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(
                c={'Name': sensor_key + '-Labels3d-PointCloud2', 'Topic': labeled_topic, 'Color': color,
                   'Color_Transformer': 'RGB8', 'Style': 'Spheres', 'Size__m_': 0.02, 'Alpha': 1})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'depth':

            # Raw image
            labeled_topic = generateLabeledTopic(topic, type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # 3D Rviz markers of Depth image 3D points corresponding to idxs and idxs_limit
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Labels-3D',
                                                                            'Marker_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        else:
            print(Fore.YELLOW + 'Warning: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                  + topic + ' (' + modality + ')' + Style.RESET_ALL)

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_dataset_playback, 'w'))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the calibrate
    # --------------------------------------------------------------------------
    print('Setting up ' + Fore.BLUE + rviz_calibrate + Style.RESET_ALL + ' ...')
    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays
    displays.pop()  # remove the last item, which is the robot model

    # Add interactive maker display for showing robot meshes
    rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        c={'Name': 'RobotMeshes-MarkerArray',
           'Marker_Topic': 'calibrate/robot_meshes'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Add a maker array display for showing the patterns
    rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        c={'Name': 'Patterns-MarkerArray',
           'Marker_Topic': 'calibrate/patterns'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Add a maker array display for miscellaneous information
    rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        c={'Name': 'Miscellaneous-MarkerArray',
           'Marker_Topic': 'calibrate/Miscellaneous'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Check if 2d lidars exist, if so add laser beams marker
    modalities = []
    for idx, sensor_key in enumerate(config['sensors']):
        modality = config['sensors'][sensor_key]['modality']
        modalities.append(modality)

    if 'lidar2d' in modalities:  # we have 2D lidar data, add laser beams marker
        # Add interactive maker display for showing laser beams
        rendered = env.get_template('/rviz/MarkerArray.rviz').render(
            c={'Name': 'LaserBeams-MarkerArray',
               'Marker_Topic': 'calibrate/LaserBeams'}
        )
        displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Generate rviz displays according to the sensor types
    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        # if topic in bag_topics:
        #     msg_type = bag_info[1][topic].msg_type
        # else:
        #     msg_type = bag_info[1][topic_compressed].msg_type
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL +
              ' with topic ' + Fore.BLUE + topic + Style.RESET_ALL + ' (' + Fore.CYAN + modality + Style.RESET_ALL + ')')

        # TODO check if depth should be here or separated
        if modality == 'rgb':  # displays for camera sensor

            # Image
            # one image per collection will be published by calibrate, but we only create one Image Display listening to the collection 0 image topic. The user can then change to other collections.
            labeled_topic = generateLabeledTopic(topic, collection_key='000', type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == "depth":

            # Depth Image with pattern idxs and idxs_limit overlayed
            labeled_topic = generateLabeledTopic(topic, collection_key='000', type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # 3D Rviz markers of Depth image 3D points corresponding to idxs and idxs_limit
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Labels-3D',
                                                                            'Marker_Topic': labeled_topic}
                                                                         )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar2d':
            labeled_topic = generateLabeledTopic(topic, type='2d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Labels3D',
                                                                            'Marker_Topic': labeled_topic}
                                                                         )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar3d':
            # 3D Rviz markers of lidar 3D points corresponding to idxs and idxs_limit
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-LabeledData',
                                                                            'Marker_Topic': labeled_topic}
                                                                         )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_calibrate, 'w'))

    bag.close()  # close the bag file.

    # Print final report
    print('\nCreated a calibration configuration summary. Use it to see if the calibration is well set up. Located at:\n' +
          Fore.BLUE + package_path + '/calibration/summary.pdf' + Style.RESET_ALL)
    print('\nSuccessfully configured calibration package ' + Fore.BLUE + package_name + Style.RESET_ALL +
          ' in ' + str(round(tictoc.tocvalue(), 2)) + ' seconds.\nYou can use the launch files:')
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(playbag_launch_file) + Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(set_initial_estimate_launch_file) +
          Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(data_collection_launch_file) +
          Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(dataset_playback_launch_file) +
          Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(calibrate_launch_file) + Style.RESET_ALL)
