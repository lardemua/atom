#!/usr/bin/env python3

# stdlib
import os
import sys
import argparse
import subprocess
from os.path import exists
from copy import deepcopy
from datetime import date, datetime

import matplotlib
import atom_core.config_io
import atom_core.drawing

# 3rd-party
import yaml
import numpy
import rospy
import rospkg
import rosbag
import jinja2
import matplotlib.pyplot as plt
import networkx as nx
from pytictoc import TicToc

# local packages
from atom_core.utilities import checkDirectoryExistence
from atom_core.naming import generateLabeledTopic
from colorama import Style, Fore
from graphviz import Digraph
from urdf_parser_py.urdf import URDF
from matplotlib import cm
from jinja2 import Environment, FileSystemLoader

if __name__ == "__main__":
    # Parse command line arguments
    ap = argparse.ArgumentParser()
    ap.add_argument("-n", "--name", help='package name', type=str, required=True)
    ap.add_argument("-utf", "--use_tfs", action="store_true",
                    help='Use transformations in the bag file instead of generating new tfs from the xacro, '
                         'joint_state_msgs and robot state publisher.')
    ap.add_argument("-cfg", "--config_file", help='Specify if you want to configure the calibration package with a specific configuration file. If this flag is not given, the standard config.yml ill be used.',
        type=str, required=False)
    args = vars(ap.parse_args())

    # --------------------------------------------------------------------------
    # Initial setup
    # --------------------------------------------------------------------------
    tictoc = TicToc()
    tictoc.tic()

    package_name = os.path.basename(args['name'])
    rospack = rospkg.RosPack()
    atom_calibration_path = rospack.get_path('atom_calibration')

    # Check if package is under $ROS_PACKAGE_PATH, abort if not
    assert (package_name in rospack.list()), \
        Fore.YELLOW + package_name + ' not found under ROS. Are you sure the path you gave in under your ' \
                                     '$ROS_PACKAGE_PATH? Calibration package will not work if it is not under the ' \
                                     '$ROS_PACKAGE_PATH. Please fix this before running the package configuration. ' \
        + Style.RESET_ALL

    package_path = rospack.get_path(package_name)  # full path to the package, including its name.
    package_base_path = os.path.dirname(package_path)  # parent path where the package is located

    rviz_file_template = atom_calibration_path + '/templates/config.rviz'
    rviz_set_initial_estimate = 'set_initial_estimate.rviz'
    rviz_collect_data = 'collect_data.rviz'
    rviz_calibrate = 'calibrate.rviz'
    rviz_dataset_playback = 'dataset_playback.rviz'
    playbag_launch_file = package_path + '/launch/playbag.launch'
    set_initial_estimate_launch_file = package_path + '/launch/set_initial_estimate.launch'
    data_collection_launch_file = package_path + '/launch/collect_data.launch'
    calibrate_launch_file = package_path + '/launch/calibrate.launch'
    dataset_playback_launch_file = package_path + '/launch/dataset_playback.launch'

    # Verify if the standard atom_calibration directories exist, if not create them (#401)
    checkDirectoryExistence('calibration', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('launch', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('rviz', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('scripts', package_name, create_if_nonexistent=True)
    checkDirectoryExistence('urdf', package_name, create_if_nonexistent=True)

    # Template engine1 setup
    file_loader = FileSystemLoader(atom_calibration_path + '/templates')
    env = Environment(loader=file_loader, undefined=jinja2.StrictUndefined)
    env.add_extension('jinja2.ext.do')

    # Date
    dt_string = datetime.now().strftime("%d/%m/%Y %H:%M:%S")

    # --------------------------------------------------------------------------
    # Read the config.yml file
    # --------------------------------------------------------------------------
    # config_file = package_path + '/calibration/config.yml'

    if args['config_file'] is None:
        args['config_file'] = package_path + '/calibration/config.yml'
    else:
        args['config_file'] = package_path + '/calibration/' + args['config_file']
        if not exists(args['config_file']):
            args['config_file'] = package_path + '/calibration/config.yml'
    print('Loading config file ' + args['config_file'])

    config = atom_core.config_io.loadConfig(args['config_file'])
    print('config_file = ' + str(args['config_file']))

    # print('Loading config file ' + config_file)
    # config = atom_core.config_io.loadConfig(config_file)

    # Sensors colormap. Access with:  color_map_sensors[idx, :]
    cm_sensors = cm.Set3(numpy.linspace(0, 1, len(config['sensors'].keys())))
    # --------------------------------------------------------------------------
    # Setup the description file
    # --------------------------------------------------------------------------
    description_file, _, _ = atom_core.config_io.uriReader(config['description_file'])
    description_file_out_initial_estimate = package_path + '/urdf/initial_estimate.urdf.xacro'
    print('description file = ' + description_file)
    atom_core.config_io.execute('cp ' + description_file + ' ' + description_file_out_initial_estimate,
                                verbose=False)  # Copy the xacro to the initial_estimate file

    # --------------------------------------------------------------------------
    # Read the bag file
    # --------------------------------------------------------------------------
    bag_file, _, bag_file_rel = atom_core.config_io.uriReader(config['bag_file'])
    print('Loading bagfile ' + bag_file)
    bag = rosbag.Bag(bag_file) # load the bag file
    bag_info = bag.get_type_and_topic_info()
    bag_types = bag_info[0]
    bag_topics = bag_info[1]
    # print('\n' + str(bag_topics))

    # --------------------------------------------------------------------------
    # Read the description.urdf.xacro file
    # --------------------------------------------------------------------------
    # Check the description file
    urdf_file = '/tmp/description.urdf'
    if os.path.exists(urdf_file):
        print('Deleting temporary file ' + urdf_file)
        os.remove(urdf_file)

    print('Parsing description file ' + description_file)
    xacro_cmd = 'xacro ' + description_file + ' -o ' + urdf_file
    atom_core.config_io.execute(xacro_cmd, verbose=True)  # create tmp urdf file

    if not os.path.exists(urdf_file):
        raise ValueError(Fore.RED + 'Could not parse description file ' + Fore.BLUE + description_file + Style.RESET_ALL + '\nYou must manually run command:\n' + Fore.BLUE + xacro_cmd + Style.RESET_ALL + '\nand fix the problem before configuring your calibration package.')

    description = URDF.from_xml_file(urdf_file)  # read the urdf file

    # --------------------------------------------------------------------------
    # Check if modalities are properly created
    # --------------------------------------------------------------------------
    # TODO change lidar to lidar3d and lidar2d to lidar2d; change modality to modality
    for sensor_key in config['sensors']:
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        if topic in bag_topics:
            msg_type = bag_info[1][topic].msg_type
        elif topic_compressed in bag_topics:
            msg_type = bag_info[1][topic_compressed].msg_type
        if ('modality' not in config['sensors'][sensor_key]) or (
                config['sensors'][sensor_key]['modality'] not in ['rgb', 'depth', 'lidar3d', 'lidar2d']):
            print(
                Fore.YELLOW + 'Warning: Sensor ID not properly identified for sensor ' + sensor_key + ' with topic '
                + topic + "Sensor will be identified by message type." + Style.RESET_ALL)
            if msg_type == 'sensor_msgs/CompressedImage' or \
                    msg_type == 'sensor_msgs/Image':
                config['sensors'][sensor_key]['modality'] = 'rgb'
            elif msg_type == 'sensor_msgs/LaserScan':
                config['sensors'][sensor_key]['modality'] = 'lidar2d'
            elif msg_type == 'sensor_msgs/PointCloud2':
                config['sensors'][sensor_key]['modality'] = 'lidar3d'
            else:
                raise ValueError(
                    Fore.RED + 'Error: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                    + topic + Style.RESET_ALL)

    # --------------------------------------------------------------------------
    # Create a tf graph to support verifications
    # --------------------------------------------------------------------------
    gx = nx.DiGraph()
    if not args['use_tfs']:  # create a graph using the information in the urdf
        print('Creating transformation tree using the urdf robot description ...')
        for link in description.links:  # A graph node for each link in the urdf
            gx.add_node(link.name)
            print(link.name)

        for joint in description.joints:  # atomic transformations are given by the joints
            if joint.type == 'fixed':
                gx.add_edge(joint.parent, joint.child, weight=1, type='static')
            else:
                gx.add_edge(joint.parent, joint.child, weight=1, type='dynamic')
    else:
        print('Creating transformation tree using the tfs in the bagfile ...')
        
        for topic, msg, t in bag.read_messages(topics=['/tf']):
            for transform in msg.transforms:
                if not gx.has_edge(transform.header.frame_id, transform.child_frame_id):
                    gx.add_edge(transform.header.frame_id, transform.child_frame_id, weight=1, type='dynamic')

        for topic, msg, t in bag.read_messages(topics=['/tf_static']):
            for transform in msg.transforms:
                if not gx.has_edge(transform.header.frame_id, transform.child_frame_id):
                    gx.add_edge(transform.header.frame_id, transform.child_frame_id, weight=1, type='static')

        # nx.draw(gx, with_labels=True)
        # plt.show()

    # --------------------------------------------------------------------------
    # Verifications: Run as much as we can think of to see if something is wrong. Better early than late.
    # --------------------------------------------------------------------------
    print('Running verifications ... please wait ...')


    # If anchored sensor exists, it must be one of the existing sensors
    if config['anchored_sensor'] != '' and \
            not config['anchored_sensor'] in list(config['sensors'].keys()):
        raise ValueError('Anchored sensor must be empty string or one of the configured sensors.')

    # Check if config sensor topics exist in the bag file (also check for compressed topics)
    compressed_topics = {}  # a list of compressed topics to decompress in the launch file
    for sensor_key in config['sensors']:
        topic = config['sensors'][sensor_key]['topic_name']
        if topic not in bag_topics:

            topic_compressed = topic + '/compressed'
            if topic_compressed in bag_topics:  # Check if the topic is a compressed image topic
                msg_type = bag_info[1][topic_compressed].msg_type
                if msg_type == 'sensor_msgs/CompressedImage':  # Check if the topic of correct msg_type
                    compressed_topics[topic] = {'topic_compressed': topic_compressed, 'msg_type': msg_type,
                                                'sensor_key': sensor_key}
                    print(Fore.BLUE + 'Topic ' + topic + ' is in compressed format (' + topic_compressed +
                          '). Will setup a decompressor.' + Style.RESET_ALL)
                else:
                    raise ValueError(Fore.RED + ' Topic ' + topic + ' (from sensor ' + sensor_key +
                                     ') exists in compressed format in the bag file, but is not of msg_type '
                                     '"sensor_msgs/CompressedImage"' + Style.RESET_ALL)
            else:

                raise ValueError(Fore.RED + ' Topic ' + topic + ' (from sensor ' + sensor_key +
                                 ') does not exist in the bag file.' + Style.RESET_ALL)

    # Check if throttled topics exist
    throttle_topics = {}
    use_throttle_topics = {}
    for sensor_key in config['sensors']:
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        if topic_compressed in bag_topics:  # Check if the topic is a compressed image topic
            topic = topic_compressed
        if 'throttle' in config['sensors'][sensor_key]:
            throttle = config['sensors'][sensor_key]['throttle']
            print('Sensor ' + sensor_key + ' has throttle')

            use_throttle = True
        else:
            print('Sensor ' + sensor_key + ' does not have throttle')
            throttle = None
            use_throttle = False
        # else:

        throttle_topics[topic] = {'throttled_topic': topic, 'sensor_key': sensor_key, 'use_throttle': use_throttle,
                                  'throttle_hz': throttle}


    # For rgb and depth sensors, check that the camera_info topic exists in the bag file
    for sensor_key in config['sensors']:
        if not config['sensors'][sensor_key]['modality'] == 'rgb' and not config['sensors'][sensor_key]['modality'] == 'depth': # no need to check for other modalities
            continue

        topic = config['sensors'][sensor_key]['topic_name']
        camera_info_topic = os.path.dirname(topic) + '/camera_info'
        if camera_info_topic not in bag_topics:
            raise ValueError(Fore.RED + ' Topic ' + camera_info_topic + ' (from sensor ' + sensor_key +
                                 ') does not exist in the bag file. This topic must exist. Aborting configuration!' + Style.RESET_ALL)

    # Verify the existence of all tf frames in the config
    frames_to_verify = {'world_link': config['world_link'],
                        'calibration_pattern.pattern_link': config['calibration_pattern']['parent_link']}
    for sensor_key in config['sensors']:  # Check if link, child_link and parent_link exist in the bagfile.
        for frame in ['child_link', 'parent_link', 'link']:
            frames_to_verify[sensor_key + '.' + frame] = config['sensors'][sensor_key][frame]

    # Verify if exist additional tfs to be estimated
    if 'additional_tfs' in config: 
        if config['additional_tfs'] != '':
            # Check if link, child_link and parent_link exist in the bagfile.
            for additional_tf_key in config['additional_tfs']:
                for frame in ['child_link', 'parent_link']:
                    frames_to_verify[additional_tf_key + '.' +
                                    frame] = config['additional_tfs'][additional_tf_key][frame]

            # Verify that the parent and child links for each additional_tfs are a direct transformation
            for additional_tf_key, additional_tf in config['additional_tfs'].items():
                edge_in_config = (config['additional_tfs'][additional_tf_key]['parent_link'],
                                config['additional_tfs'][additional_tf_key]['child_link'])

                if not edge_in_config in gx.edges():
                    raise ValueError(
                        'Error: additional_tfs ' + Fore.BLUE + additional_tf_key + Style.RESET_ALL + ' parent and child links are ' + Fore.YELLOW +
                        edge_in_config[0] + Fore.BLACK + ' and ' + Fore.YELLOW + edge_in_config[1] + Fore.BLACK +
                        ' but there is no such atomic transformation in the tf tree.' + Style.RESET_ALL)

            for additional_tf_key, additional_tf in config['additional_tfs'].items():
                # path between root and additional_tfs data frame
                paths = nx.shortest_path(gx, config['world_link'], additional_tf['child_link'])
                print('Path from ' + Fore.RED + config['world_link'] + Fore.RESET + ' to additional_tfs ' + Fore.LIGHTMAGENTA_EX +
                    additional_tf_key + Fore.RESET + ':')
                at = '['
                for path in paths:
                    if path == config['world_link']:
                        at += Fore.RED + path + Fore.RESET + ', '
                    elif path == additional_tf['parent_link'] or path == additional_tf['child_link']:
                        at += Fore.BLUE + path + Fore.RESET + ', '
                    else:
                        for joint in description.joints:  # atomic transformations are given by the joints
                            if path == joint.parent or path == joint.child:
                                if joint.type == 'fixed':
                                    at += path + ', '
                                else:
                                    at += Fore.GREEN + path + Fore.RESET + ', '
                                break
                at = at[:-2]
                at += ']'
                print(at)

    print('Checking if all tf frames exist ...')
    print(frames_to_verify)
    for key, frame in frames_to_verify.items():
        if not frame in gx:
            raise ValueError('Error: frame ' + Fore.RED + frame + Style.RESET_ALL + ' given in config file for ' +
                             Fore.BLUE + key + Style.RESET_ALL + ' does not exist. Check config file ' +
                             Fore.BLUE + args['config_file'] + Style.RESET_ALL)

    # Verify that the parent and child links for each sensor are a direct transformation (#316).
    for sensor_key, sensor in config['sensors'].items():
        edge_in_config = (config['sensors'][sensor_key]['parent_link'], config['sensors'][sensor_key]['child_link'])
        if not edge_in_config in gx.edges():
            raise ValueError(
                'Error: sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL + ' parent and child links are ' + Fore.YELLOW +
                edge_in_config[0] + Fore.BLACK + ' and ' + Fore.YELLOW + edge_in_config[1] + Fore.BLACK +
                ' but there is no such atomic transformation in the tf tree.' + Style.RESET_ALL)

    # Verify that the frame_id in the calibration.yaml->sensor->link field is the same as in the published messages (#514).
    print('Checking consistency of configuration tf frames with the messages on corresponding topic ...')
    for sensor_key, sensor in config['sensors'].items():
        frame_in_config = config['sensors'][sensor_key]['link']
        topic_in_config = config['sensors'][sensor_key]['topic_name']

        for topic, msg, t in bag.read_messages(topics=[topic_in_config]):
            if not msg.header.frame_id == frame_in_config:
                print('Error: sensor ' + Fore.BLUE + sensor_key + Style.RESET_ALL + ' calibration config defined link ' + Fore.YELLOW + frame_in_config + Fore.BLACK +' but bagfile messages on topic ' + Fore.BLUE + topic_in_config + Fore.BLACK + ' have header.frame_id ' + Fore.YELLOW + msg.header.frame_id + Fore.BLACK + '. These should be the same! Check #514.'+ Style.RESET_ALL)
                exit(0)
            break # check consistency only for the first message

    # if nx.is_connected(gx):
    if nx.is_weakly_connected(gx):
        print('TF tree is connected.')
    else:
        pos = nx.random_layout(gx)
        edges, weights = zip(*nx.get_edge_attributes(gx, 'weight').items())
        edge_labels = nx.draw_networkx_edge_labels(gx, pos)
        nx.draw(gx, with_labels=True)
        plt.show()
        raise ValueError('TF tree is not connected. Aborting.')

    # Pretty print of transformation chains
    # TODO How to detect a dynamic transformation using the information in the bagfile's tfs?
    print(
        'Transformations for sensors. Color coding is ' + Fore.RED + 'root link' + Fore.RESET + ', ' + Fore.BLUE + 'to be calibrated'
        + Fore.RESET + ', ' + Fore.GREEN + 'dynamic transformation' + Fore.RESET + ', ' + Fore.LIGHTMAGENTA_EX +
        'sensor data frame' + Fore.RESET + '.')
    for sensor_key, sensor in config['sensors'].items():
        paths = nx.shortest_path(gx, config['world_link'], sensor['link'])  # path between root and sensor data frame
        print('Path from ' + Fore.RED + config['world_link'] + Fore.RESET + ' to sensor ' + Fore.LIGHTMAGENTA_EX +
              sensor_key + Fore.RESET + ':')
        s = '['
        for path in paths:
            if path == config['world_link']:
                s += Fore.RED + path + Fore.RESET + ', '
            elif path == sensor['parent_link'] or path == sensor['child_link']:
                s += Fore.BLUE + path + Fore.RESET + ', '
            elif path == sensor['link']:
                s += Fore.LIGHTMAGENTA_EX + path + Fore.RESET + ', '
            else:
                for joint in description.joints:  # atomic transformations are given by the joints
                    if path == joint.parent or path == joint.child:
                        if joint.type == 'fixed':
                            s += path + ', '
                        else:
                            s += Fore.GREEN + path + Fore.RESET + ', '
                        break
        s = s[:-2]
        s += ']'
        print(s)

    # --------------------------------------------------------------------------
    # Create dot calibration graph (used for printing out a summary)
    # --------------------------------------------------------------------------
    g = Digraph('G', filename='summary', directory='/tmp/', graph_attr={'title': 'graph'})  # create a graph

    for link in gx.nodes:  # A graph node for each link in the urdf
        rgb = matplotlib.colors.rgb2hex([0, 0, 0])  # black by default
        label = link
        if config['world_link'] == link:
            label += '\n(world link)'
            rgb = matplotlib.colors.rgb2hex([1, 0, 0])

        for sensor_key, sensor in config['sensors'].items():
            if sensor['link'] == link:
                label += '\n(data from sensor ' + sensor_key + ')'
                rgb = matplotlib.colors.rgb2hex([0, 0.5, 0])

        g.node(link, label=label, _attributes={'penwidth': '2', 'color': rgb}, )

    for parent, child, edge_data in gx.edges(data=True):  # atomic transformations are given by the joints
        label = ''
        type = edge_data['type']
        rgb = matplotlib.colors.rgb2hex([0, 0, 0])  # black by default
        for sensor_key, sensor in config['sensors'].items():  # if the joint is marked for calibration
            if sensor['parent_link'] == parent and sensor['child_link'] == child:
                label += 'To be calibrated\n'
                rgb = matplotlib.colors.rgb2hex([0, 0, 1])

        if 'additional_tfs' in config: 
            if config['additional_tfs'] != '':
                for additional_tfs_key, additional_tfs in config['additional_tfs'].items():
                    if additional_tfs['parent_link'] == parent and additional_tfs['child_link'] == child:
                        label += 'To be calibrated\n'
                        rgb = matplotlib.colors.rgb2hex([0, 0, 1])
                        
        label += type

        g.edge(parent, child, color=rgb, style='solid', _attributes={'penwidth': '1', 'fontcolor': rgb},
               label=label)

    # g.view()
    g.render(filename='summary', directory=package_path + '/calibration', cleanup=True)

    # --------------------------------------------------------------------------
    # Create the playback launch file
    # --------------------------------------------------------------------------
    launch_file = playbag_launch_file
    print('Setting up ' + playbag_launch_file + ' ...')

    template = env.get_template('playbag.launch')
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'bag_file': bag_file_rel,
                                    'use_tfs': args['use_tfs'],
                                    'package_name': package_name,
                                    'rviz_set_initial_estimate': rviz_set_initial_estimate,
                                    'use_compressed_topics': bool(compressed_topics),
                                    'compressed_topics': compressed_topics,
                                    'throttle_topics': throttle_topics
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the set_initial_estimate launch file
    # --------------------------------------------------------------------------
    launch_file = set_initial_estimate_launch_file
    print('Setting up ' + launch_file + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'rviz_set_initial_estimate': rviz_set_initial_estimate,
                                    'bag_file': bag_file_rel,
                                    'marker_size': 0.5
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the data collection launch file
    # --------------------------------------------------------------------------
    launch_file = data_collection_launch_file
    print('Setting up ' + launch_file + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'rviz_collect_data': rviz_collect_data,
                                    'bag_file': bag_file_rel,
                                    'config_file': os.path.basename(args['config_file'])
                                    })
        f.write(output.encode('utf-8'))

    # --------------------------------------------------------------------------
    # Create the calibrate launch file
    # --------------------------------------------------------------------------
    launch_file = calibrate_launch_file
    print('Setting up ' + launch_file + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'rviz_calibrate': rviz_calibrate
                                    })
        f.write(output.encode('utf-8'))
    # --------------------------------------------------------------------------
    # Create the dataset playback launch file
    # --------------------------------------------------------------------------
    launch_file = dataset_playback_launch_file
    print('Setting up ' + launch_file + ' ...')

    template = env.get_template(os.path.basename(launch_file))
    with open(launch_file, 'wb') as f:
        output = template.render(c={'filename': os.path.basename(launch_file),
                                    'date': dt_string,
                                    'package_name': package_name,
                                    'bag_file': bag_file_rel,
                                    'rviz_dataset_playback': rviz_dataset_playback
                                    })
        f.write(output.encode('utf-8'))
   
    # --------------------------------------------------------------------------
    # Create the rviz config core displays (used in several rviz config files)
    # --------------------------------------------------------------------------
    # TODO change rviz fixed_frame according to args['world_link']
    core_displays = []

    # Create grid, tf and robot model displays
    rendered = env.get_template('/rviz/Grid.rviz').render(c={'Reference_Frame': config['world_link']})
    core_displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    rendered = env.get_template('/rviz/TF.rviz').render(c={'Name': 'TF'})
    core_displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    rendered = env.get_template('/rviz/RobotModel.rviz').render(c={'Name': 'RobotModel'})
    core_displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the set_initial_estimate
    # --------------------------------------------------------------------------
    print('Setting up ' + rviz_set_initial_estimate + ' ...')
    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays

    # Create interactive marker display for moving the sensors
    rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(c={'Name': 'MoveSensors-InteractiveMarker',
                                                                          'Update_Topic': 'set_initial_estimate/update'}
                                                                       )
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Generate rviz displays according to the sensor types
    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        # if topic in bag_topics:
        #     msg_type = bag_info[1][topic].msg_type
        # else:
        #     msg_type = bag_info[1][topic_compressed].msg_type
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + sensor_key + ' with topic ' + topic + ' (' + modality + ')')

        # if msg_type == 'sensor_msgs/CompressedImage' or \
        #         msg_type == 'sensor_msgs/Image':  # add displays for camera sensor
        # TODO check if depth should be here or not
        if modality == 'rgb' or modality == 'depth':

            # Raw image
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Image',
                                                                      'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Camera
            rendered = env.get_template('/rviz/Camera.rviz').render(c={'Name': sensor_key + '-Camera',
                                                                       'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Frustum
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Frustum',
                                                                            'Marker_Topic': sensor_key + '/frustum'})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))


        # elif msg_type == 'sensor_msgs/LaserScan':
        elif modality == 'lidar2d':
            # Raw data
            rendered = env.get_template('/rviz/LaserScan.rviz').render(c={'Name': sensor_key + '-LaserScan',
                                                                          'Topic': topic,
                                                                          'Color': color})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/PointCloud2':
        elif modality == 'lidar3d':
            # Raw data
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-PointCloud2',
                                                                            'Topic': topic,
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.02,
                                                                            'Alpha': 1})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        else:
            print(Fore.YELLOW + 'Warning: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                  + topic + ' (' + modality + ')' + Style.RESET_ALL)

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_set_initial_estimate, 'w'))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the collect_data
    # --------------------------------------------------------------------------
    print('Setting up ' + rviz_collect_data + ' ...')
    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays

    # Add interactive maker display to handle sensor manual labeling
    rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
        c={'Name': 'ManualDataLabeler-InteractiveMarkers',
           'Update_Topic': 'data_labeler/update'}
    )
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))
    # TODO this will be the menu (only one)

    depth = 0
    # Generate rviz displays according to the sensor types
    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        # if topic in bag_topics:
        #     msg_type = bag_info[1][topic].msg_type
        # else:
        #     msg_type = bag_info[1][topic_compressed].msg_type
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + sensor_key + ' with topic ' + topic + ' (' + modality + ')')

        # if msg_type == 'sensor_msgs/CompressedImage' or msg_type == 'sensor_msgs/Image':  # displays for camera sensor
        # TODO check if depth should be here or separated
        if modality == 'rgb':
            # Image
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': topic + '/labeled'})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Camera
            rendered = env.get_template('/rviz/Camera.rviz').render(c={'Name': sensor_key + '-Camera',
                                                                       'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))


        elif modality == 'depth':
            depth += 1
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': topic + '/labeled'})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Camera
            rendered = env.get_template('/rviz/Camera.rviz').render(c={'Name': sensor_key + '-Camera',
                                                                       'Image_Topic': topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
                c={'Name': sensor_key + '-ManualDataLabeler-InteractiveMarkers',
                   'Update_Topic': sensor_key + '/data_labeler/update'}
            )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/LaserScan':
        elif modality == 'lidar2d':
            rendered = env.get_template('/rviz/LaserScan.rviz').render(c={'Name': sensor_key + '-LaserScan',
                                                                          'Topic': topic,
                                                                          'Color': color})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Labels
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-Labels-PointCloud2',
                                                                            'Topic': topic + '/labeled',
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.2,
                                                                            'Alpha': 0.05})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # TODO Add markers to display data clusters

            rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
                c={'Name': sensor_key + '-ManualDataLabeler-InteractiveMarkers',
                   'Update_Topic': sensor_key + '/data_labeler/update'}
            )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        # elif msg_type == 'sensor_msgs/PointCloud2':
        elif modality == 'lidar3d':
            # Raw data
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-PointCloud2',
                                                                            'Topic': topic,
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.02,
                                                                            'Alpha': 1})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Labeled data
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(c={'Name': sensor_key + '-Labels-PointCloud2',
                                                                            'Topic': topic + '/labeled',
                                                                            'Color': color,
                                                                            'Color_Transformer': 'FlatColor',
                                                                            'Style': 'Spheres',
                                                                            'Size__m_': 0.2,
                                                                            'Alpha': 0.05})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # Interactive marker for the labeler
            rendered = env.get_template('/rviz/InteractiveMarker.rviz').render(
                c={'Name': sensor_key + '-ManualDataLabeler-InteractiveMarkers',
                   'Update_Topic': sensor_key + '/data_labeler/update'}
            )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        else:
            print(Fore.YELLOW + 'Warning: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                  + topic + ' (' + modality + ')' + Style.RESET_ALL)

    if depth != 0:  # TODO #411 this is strange here ...
        # rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        #     c={'Name': 'FrustumVisualizationDepthSensors',
        #        'Marker_Topic': '/sensor_frustum'})
        # displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))
        pass

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_collect_data, 'w'))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the dataset_playback
    # --------------------------------------------------------------------------
    print('Setting up ' + rviz_dataset_playback + ' ...')
    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays
    displays.pop()  # remove last core display which is robot model

    rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': 'Robot Meshes',
                                                                    'Marker_Topic': '/dataset_playback/robot_meshes'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + sensor_key + ' with topic ' + topic + ' (' + modality + ')')

        if modality == 'rgb':
            # Raw image
            labeled_topic = generateLabeledTopic(topic, type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar2d':
            # Raw data
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/LaserScan.rviz').render(c={'Name': sensor_key + '-Labels-LaserScan',
                                                                          'Topic': labeled_topic,
                                                                          'Color': color})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar3d':
            # Raw data
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/PointCloud2.rviz').render(
                c={'Name': sensor_key + '-Labels3d-PointCloud2', 'Topic': labeled_topic, 'Color': color,
                   'Color_Transformer': 'RGB8', 'Style': 'Spheres', 'Size__m_': 0.02, 'Alpha': 1})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'depth':

            # Raw image
            labeled_topic = generateLabeledTopic(topic, type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # 3D Rviz markers of Depth image 3D points corresponding to idxs and idxs_limit
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Labels-3D',
                                                                            'Marker_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        else:
            print(Fore.YELLOW + 'Warning: Cannot generate rviz configuration for sensor ' + sensor_key + ' with topic '
                  + topic + ' (' + modality + ')' + Style.RESET_ALL)

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_dataset_playback, 'w'))

    # --------------------------------------------------------------------------
    # Create the rviz config file for the calibrate
    # --------------------------------------------------------------------------
    print('Setting up ' + rviz_calibrate + ' ...')
    rviz = yaml.load(open(rviz_file_template), Loader=yaml.SafeLoader)
    rviz['Visualization Manager']['Global Options']['Fixed Frame'] = config[
        'world_link']  # set fixed frame to world_link
    displays = deepcopy(core_displays)  # start with the core displays
    displays.pop()  # remove the last item, which is the robot model

    # Add interactive maker display for showing robot meshes
    rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        c={'Name': 'RobotMeshes-MarkerArray',
           'Marker_Topic': 'calibrate/robot_meshes'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Add a maker array display for showing the patterns
    rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        c={'Name': 'Patterns-MarkerArray',
           'Marker_Topic': 'calibrate/patterns'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Add a maker array display for miscellaneous information
    rendered = env.get_template('/rviz/MarkerArray.rviz').render(
        c={'Name': 'Miscellaneous-MarkerArray',
           'Marker_Topic': 'calibrate/Miscellaneous'})
    displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Check if 2d lidars exist, if so add laser beams marker
    modalities = []
    for idx, sensor_key in enumerate(config['sensors']):
        modality = config['sensors'][sensor_key]['modality']
        modalities.append(modality)

    if 'lidar2d' in modalities:  # we have 2D lidar data, add laser beams marker
        # Add interactive maker display for showing laser beams
        rendered = env.get_template('/rviz/MarkerArray.rviz').render(
            c={'Name': 'LaserBeams-MarkerArray',
               'Marker_Topic': 'calibrate/LaserBeams'}
        )
        displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    # Generate rviz displays according to the sensor types
    # Iterate alphabetically through the sensors dict keys https://github.com/lardemua/atom/issues/409
    for idx, sensor_key in enumerate(sorted(list(config['sensors'].keys()))):
        color = atom_core.drawing.colormapToRVizColor(cm_sensors[idx, :])
        topic = config['sensors'][sensor_key]['topic_name']
        topic_compressed = topic + '/compressed'
        # if topic in bag_topics:
        #     msg_type = bag_info[1][topic].msg_type
        # else:
        #     msg_type = bag_info[1][topic_compressed].msg_type
        modality = config['sensors'][sensor_key]['modality']

        print('\tGenerating rviz displays for sensor ' + sensor_key + ' with topic ' + topic + ' (' + modality + ')')
        # TODO check if depth should be here or separated
        if modality == 'rgb':  # displays for camera sensor

            # Image
            # one image per collection will be published by calibrate, but we only create one Image Display listening to the collection 0 image topic. The user can then change to other collections.
            labeled_topic = generateLabeledTopic(topic, collection_key='000', type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == "depth":

            # Depth Image with pattern idxs and idxs_limit overlayed
            labeled_topic = generateLabeledTopic(topic, collection_key='000', type='2d')
            rendered = env.get_template('/rviz/Image.rviz').render(c={'Name': sensor_key + '-Labels' + '-Image',
                                                                      'Image_Topic': labeled_topic})
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

            # 3D Rviz markers of Depth image 3D points corresponding to idxs and idxs_limit
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Labels-3D',
                                                                            'Marker_Topic': labeled_topic}
                                                                         )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar2d':
            labeled_topic = generateLabeledTopic(topic, type='2d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-Labels3D',
                                                                            'Marker_Topic': labeled_topic}
                                                                         )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

        elif modality == 'lidar3d':
            # 3D Rviz markers of lidar 3D points corresponding to idxs and idxs_limit
            labeled_topic = generateLabeledTopic(topic, type='3d')
            rendered = env.get_template('/rviz/MarkerArray.rviz').render(c={'Name': sensor_key + '-LabeledData',
                                                                            'Marker_Topic': labeled_topic}
                                                                         )
            displays.append(yaml.load(rendered, Loader=yaml.SafeLoader))

    rviz['Visualization Manager']['Displays'] = displays
    yaml.dump(rviz, open(package_path + '/rviz/' + rviz_calibrate, 'w'))

    bag.close() # close the bag file.

    # Print final report
    print('\nCreated a calibration configuration summary. Use it to see if the calibration is well set up. Located at:\n' + package_path + '/calibration/summary.pdf')
    print('\nSuccessfully configured calibration package ' + Fore.BLUE + package_name + Style.RESET_ALL + ' in ' + str(round(tictoc.tocvalue(),2)) + ' seconds.\nYou can use the launch files:')
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(playbag_launch_file) + Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(set_initial_estimate_launch_file) +
          Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(data_collection_launch_file) +
          Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(dataset_playback_launch_file) +
          Style.RESET_ALL)
    print(Fore.BLUE + 'roslaunch ' + package_name + ' ' + os.path.basename(calibrate_launch_file) + Style.RESET_ALL)
